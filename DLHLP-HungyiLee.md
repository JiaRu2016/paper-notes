
[youtebe](https://www.youtube.com/channel/UC2ggjtuuWvxrHHHiaDH1dlQ) 
slides under each video

## Introduction

六种任务

- autio -> text. speech recognization. 语音识别  **It's not the seq2seq you know!**
-  autio -> audio. 
    + Speech Separation 从背景音中分离出语音、两个人说话分离
    + Voice Conversion 把一个人的声音变成柯南or新垣结衣. 难点：one-shot
- autio -> class.
    + which speaker?
    + keyword spotting.
        * Wake up words **需要模型非常轻量**
- text -> audio. Text to speech synthesis 语音合成
- text -> text. Translation, Summarization, Chat-bot, QA
- text -> class.

”硬train一发“的问题：label非常昂贵或不可得


## Speech Recognization (ASR)

### input output

input: audio `(T_audio, D)`
output: sequence of tokens `(T_token, )`, token vocab size = `V`

输出形式有哪些选择？

- phoneme 发音最小单位，“音素”
    + Need **Lexicon** (mapping of word to seq of phoneme, languagist expert knowledge), which is expensive for some language
    + 声音和音素是一一对应的，比较好学
- grapheme 书写最小单位。
    + 英文：26 characters + space；中文 V=4000+
    + 声音和书写单位不是一一对应的，例如英文中发音'k'可以是k或c，中文的多音字。需要上下文信息， maybe languge model?
- morpheme: smallest meaningful unit, eg. "un, break, able", "re, kill, able"
    + 可以用语言学知识，也可以从大量语料库中用统计方法找pattern
- word: 中文 V=??
- byte: utf8 encoding. Language independent!

输入声学特征 `shape=(T, D)`

滑动窗口 eg. 10ms, 25ms => 100 frames ie. T=100 per second

waveform -(DFT)-> spectgram -> FilterBank -> log -> MFCC

- 原始声音 sample points
- log filter bank outputs `D=80`
- MFCC, `D=39`

### eval metric

- LAS: WER word error rate: = (S + I + D) / N
- CTC: LER label error rate: edit distance ED(p, q) = number of insertion, substitution, and deletion requied to change p to q

### LAS: typical seq2seq with attention

*Listen, Attend, and Spell (LAS)*

$$
h = Listen( x) \\
y_t = AttendAndSpell(h, y_{<t}) \\
$$

Listen: RNN (possiblely with pooling or down-sampling) or TransformerEncoder. pyrimid-BLSTM

AttendAndSpell: 
$$
s_t = RNN(s_{t-1}, c_{t-1}, y_{t-1}) \\
c_t = AttentionContext(h, s_t) \\
y_t = f(c_t, s_t)
$$

Beam search. hyperparam beam_width B, prune the V ary tree to make number of leaf always B.

Teacher Forcing: 如果用 $\hat y_{t-1}$ 当做RNN输入，那实际上在t>1位置上给了RNN非常多错误的训练样本。 应该用 ground truth $y_{t-1}$

关于Attention的讨论：Translation任务确实需要attention, 因为src和target位置对应关系不一定是顺序的，src最后一个单词可能对应target第一个单词；而语音识别任务则不是，位置对应关系应该是顺序的，所以有 location-aware attention

Language Model rescoring (over top 32 beam)

$$
s(y|x) = 
    \frac{\log P(y|x)}{|y|_c} + 
    \lambda \log P_{LM}(y|x)
$$


### CTC: all possible allignment as "smoothed" target

我的理解： Solving the problem of inconsistant `T` by introducing "blank" token and make all possiable alligenment as kind of "smoothed" or "mixed" target which kept order of ground truth target while allow absolute positions of char to vary.

[An Intuitive Explanation of CTC](https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c)


pytorch CTCLoss interface: `ctc_loss_fn(log_probs, target)`

```python
# model output: log_probs (T, vocab_size)
# target (T_target, )
vocab_size = 3000
T = 100
max_T_target = 20
bz = 32

log_probs = torch.randn(T, bz, vocab_size).log_softmax(dim=2)  # (T, N, C)
target = torch.randint(high=vocab_size, size=(bz, max_T_target))   # (N, S)

nn.CTCLoss(blank=0)(
    log_probs, target,
    input_lengths=torch.full((bz,), T),
    target_lengths=torch.randint(max_T_target, (bz, ))
)
```

math: 
* maximize scores of all possiable path, where 
    - "all possiable path" is generated by adding "blank" token and duplicating some token
    - score of a path is prod of each token prob
* eaquivalent to minimize sum negative log prob


## Voice Conversion

1. feature disentangle: Content Encoder, Speaker Encoder and Decoder.
2. Directly transform: GAN

### feature disentangle

Content/Speaker Encoder and Decoder.

how to train content Encoder? 
- pretrained ASR
- Advserial training Discriminator of speaker

how to train Speaker Encoder? classification of speaker

train Decoder: reconstruct loss

### Directly transform: CycleGAN and StarGAN

- (Conditional)Generator input voice and speaker label, oupput voice
    + "Cycle": i -> j -> i reconstruct loss
- Discriminator inpout voice and speaker label, ouput is fake or real (ie. is the voice belongs to the speaker)

my impl (TO VERIFY)
```python
v1 = G(v0, speaker_i)
v2 = G(v1, spreaker_j)

with G.training() and D.freezed():
    reconstruct_loss(v2, v0).backward()
    bce_loss(D(v1, spreaker_i), ONE).backward()

with D.training() and G.freezed():
    bce_loss(D(v1, spreaker_i), ZERO).backward()
```


## Extraction-based QA 

### SQuAD dataset

use torchtext to download

torchtext dataset yielding batch:
```python
ds_train, ds_dev = torchtext.datasets.SQuAD2(
    root='/home/rjia/playground/datasets_torch', 
    split=('train', 'dev')
)
# len(ds_train) / 1e4   # 13w

for batch in ds_train:
    doc, q, a, start = batch
    break

print(doc, '--------', q , a, start, sep='\n')
```
```
Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record prod ... singles "Crazy in Love" and "Baby Boy".
--------
When did Beyonce start becoming popular?
['in the late 1990s']
[269]
```

raw data struct:

```python
with open('/home/rjia/playground/datasets_torch/SQuAD2/train-v2.0.json', 'r') as f:
    d = json.load(f)

# json file -> dict d
# d['data'][0]
{
    'title': str,    # one document
    'paragraphs': [
        # many paragraphs, 
        {
            'context': str,  # very long
            'qas': [
                # each paragraph with many question
                {
                    'question': str, 
                    'id': str, 
                    'answers': [{'text': str, 'start': int}],  # each question with many answers
                    'is_impossible': bool
                },
                # ...
            ]
        },
        # ...
    ]
}
```

### eval metrics: F1 and EM(exact match)

- 所有answers取最大的那个
- F1 score: 将单词打碎，当做词袋模型，计算 precision and recall。 F1 = 调和平均数(precision, recall)
- EM: 精确匹配 1.0 or 0.0


### Model: Attention(Query, Context) and Context self-Attention

[](./fig/ExtactionQA_qcAttn.jpeg)

Before BERT: 

```python
# (dim for one sample)
# document: (T_doc, d)
# question: (T_qs, d)

# Query-to-Context attention --------------
query_summary = QuestionEncoder(question)  # (D,)
extracted = Attntion(
    q=query_summary, 
    k=Kproj(document), 
    v=Vporj(document)
)  # (D, )
answer = AnswerModule(extracted) # classifiction scaler or tuple (start, end)

# Context-to-Query attention --------------
for doc_t in document:   # loop dim=T
    extracted_t = Attntion(q=Qproj(doc_t), k_and_v=QuestionEncoder(question)) # (D,)
    doc_merge_query_t = MergeModule(extracted_t, doc_t)  # (D,)

doc_merge_query = Concat(doc_merge_query_t, dim=t)  # (T_doc, D)
answer = AnswerModule(doc_merge_query)

# BiDAF: Bi-Directional Attention Flow
# self-attention over document, stacking over Context-Query Attention Flow
Attention(q_k_v=doc_merge_query or document)
```

BERT: 全都要

```python
Attention([document, '[SEP]', query])
```

### Engineering chanlenge: too long documnet

TODO